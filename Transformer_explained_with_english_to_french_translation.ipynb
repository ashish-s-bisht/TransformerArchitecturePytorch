{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY2181W_vsto"
      },
      "source": [
        "# Transformer complete code for English  to French translation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fetching English-French Dataset\n",
        "!mkdir data\n",
        "!wget -O data/train.en.txt https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.en\n",
        "!wget -O data/train.fr.txt https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/train.lc.norm.tok.fr\n",
        "!wget -O data/val.en.txt https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/val.lc.norm.tok.en\n",
        "!wget -O data/val.fr.txt https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/val.lc.norm.tok.fr\n",
        "!wget -O data/test.en.txt https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/test_2017_mscoco.lc.norm.tok.en\n",
        "!wget -O data/test.fr.txt https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok/test_2017_mscoco.lc.norm.tok.fr"
      ],
      "metadata": {
        "id": "haueTtn6GFFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rS2U-OxlvsKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99904a8d-63f4-4ae3-9bfb-05a012cc4b48",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m628.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Library imports\n",
        "!pip install portalocker --quiet\n",
        "!pip install sacrebleu --quiet\n",
        "!pip install sentencepiece --quiet\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sacrebleu\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The class represents a multi-headed attention mechanism used in the Transformer model for capturing dependencies between different parts of the input sequence.**"
      ],
      "metadata": {
        "id": "ZCs0jfLUdZTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LmEC70bPV0lE"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, query, keys, values, mask):\n",
        "        # Get number of training examples\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Linear transformations for values, keys, and queries\n",
        "        values = self.values(values)  # (N, value_len, embed_size)\n",
        "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
        "        queries = self.queries(query)  # (N, query_len, embed_size)\n",
        "\n",
        "        # Split the embedding into different heads pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim) # (N, value_len, heads, embed_size)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim) # (N, key_len, heads, embed_size)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim) # (N, query_len, heads, embed_size)\n",
        "\n",
        "        #N*H,Q,D\n",
        "        queries_reshaped = queries.permute(0, 2, 1, 3).contiguous().view(-1, queries.size(1), queries.size(-1))\n",
        "        #N*H,K,D\n",
        "        keys_reshaped = keys.permute(0, 2, 1, 3).contiguous().view(-1, keys.size(1), keys.size(-1))\n",
        "\n",
        "        # Compute energy (query-key interaction)\n",
        "        #N*H,Q,K\n",
        "        energy = torch.matmul(queries_reshaped, keys_reshaped.transpose(1, 2))\n",
        "        #N,H,Q,K\n",
        "        energy = energy.view(queries.size(0), queries.size(2), queries.size(1), keys.size(1))\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == False, float(\"-inf\"))\n",
        "\n",
        "        # Apply softmax to obtain attention weights\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "\n",
        "        #N,H,V,D\n",
        "        values_reshaped = values.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # Compute the weighted sum using attention weights\n",
        "        #N,Q,H,D\n",
        "        out = torch.matmul(attention, values_reshaped).permute(0,2,1,3).contiguous().reshape(N, query_len,self.heads * self.head_dim)\n",
        "\n",
        "        #N,Q,embed_size\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This TransformerBlock represents a single block within the Transformer model, containing an attention mechanism, residual network, normalization, a feedforward network, and dropout for regularization.**"
      ],
      "metadata": {
        "id": "X9EZt-XEbbeY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "unMhi9zNV7uU"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadedAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        # Feedforward network with expansion for introducing non-linearity\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(expansion * embed_size, embed_size),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        # Attention mechanism using MultiHeadedAttention\n",
        "        attention = self.attention(query, key, value, mask)\n",
        "        # Residual connection and normalization for the first stage\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        # Feedforward network\n",
        "        forward = self.feed_forward(x)\n",
        "        # Residual connection and normalization for the second stage\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This Encoder class represents the encoder component of a Transformer model, incorporating word embeddings, positional embeddings, and multiple layers of Transformer blocks for encoding the input sequence.**"
      ],
      "metadata": {
        "id": "PUkLEw2ecIUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a7FyDKdAWBSU"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__( self, src_vocab_size, embed_size, num_layers,\n",
        "                 heads, forward_expansion, dropout, max_length):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        # Stacked Transformer blocks for encoding the input sequence\n",
        "        self.layers = nn.ModuleList([\n",
        "                TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "                for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        N, seq_length = x.shape\n",
        "        # Generate positional indices for the input sequence\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(DEVICE)\n",
        "         # Apply dropout to the sum of word and positional embeddings\n",
        "        x = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions)))\n",
        "        # Pass the input through stacked Transformer blocks\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x, x, mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The DecoderBlock represents a block within the decoder, and the Decoder class represents the decoder component of a Transformer model.**"
      ],
      "metadata": {
        "id": "sZ860wPvcMp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r_fp45rOWGmY"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.attention = MultiHeadedAttention(embed_size, heads=heads)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, heads, dropout, forward_expansion\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, value, key, src_mask, trg_mask):\n",
        "        # Self-attention mechanism for decoding\n",
        "        attention = self.attention(query, query, query, trg_mask)\n",
        "        # Residual connection and normalization for the self-attention output\n",
        "        query = self.dropout(self.norm(attention + query))\n",
        "        # Pass through the transformer block for further processing\n",
        "        out = self.transformer_block(query, key, value, src_mask)\n",
        "\n",
        "        return out\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers,\n",
        "        heads, forward_expansion, dropout, max_length):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "        # Stacked DecoderBlocks for decoding the target sequence\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderBlock(embed_size, heads, forward_expansion, dropout)\n",
        "                for _ in range(num_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "\n",
        "        N, seq_length = x.shape\n",
        "        # Generate positional indices for the input sequenc\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(DEVICE)\n",
        "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "        # Pass through stacked DecoderBlocks for decoding\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "        # Output through fully connected layer\n",
        "        out = self.fc_out(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Transformer class represents the overall Transformer model, and the ModelBuilder class is a utility class for building models with specific configurations.**"
      ],
      "metadata": {
        "id": "JuTQHl-Dcee_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a0NG_SPCWRjl"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__( self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
        "                 embed_size=512, num_layers=6, forward_expansion=4, heads=8, dropout=0.1, max_length=100):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "        # Instantiate the Encoder and Decoder\n",
        "        self.encoder = Encoder( src_vocab_size, embed_size, num_layers, heads,\n",
        "            forward_expansion, dropout, max_length)\n",
        "\n",
        "        self.decoder = Decoder( trg_vocab_size, embed_size, num_layers, heads,\n",
        "            forward_expansion, dropout, max_length)\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # Create a mask to ignore padding tokens in the source sequence\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(DEVICE)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        # Create a mask to ignore padding tokens and future tokens in the target sequence\n",
        "        trg_mask1 = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask2 = torch.tril(torch.ones((trg_len, trg_len), dtype=bool)\n",
        "        ).expand(N, 1, trg_len,trg_len).to(DEVICE)\n",
        "        trg_mask = trg_mask1 & trg_mask2\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Generate source and target masks\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        # Pass the source sequence through the encoder\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        # Pass the target sequence and encoded source sequence through the decoder\n",
        "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return out\n",
        "\n",
        "class ModelBuilder:\n",
        "    def __init__(self, model_config):\n",
        "        self.model_config = model_config\n",
        "    def make_model(self):\n",
        "        model = Transformer(**self.model_config).to(DEVICE)\n",
        "        # Initialize model parameters using Xavier uniform initialization\n",
        "        for p in model.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following code blocks contain code for Multi30k English-French dataset preparation and translation training using the above model architecture.**"
      ],
      "metadata": {
        "id": "pkaWNv-vdj0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_juZDlG8oTFh"
      },
      "outputs": [],
      "source": [
        "SRC = 'en'\n",
        "TRG = 'fr'\n",
        "en_vocab_size = 6261\n",
        "fr_vocab_size = 6261\n",
        "vocab_sizes = {\"en\": en_vocab_size, \"fr\": fr_vocab_size}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1fYsTxiEpfu2"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.train\\\n",
        "(f'--input=data//train.en.txt --model_prefix=Multi30k_en --user_defined_symbols=<pad> --vocab_size=6261')\n",
        "spm.SentencePieceTrainer.train\\\n",
        "(f'--input=data//train.fr.txt --model_prefix=Multi30k_fr --user_defined_symbols=<pad> --vocab_size=6261')\n",
        "\n",
        "en_sp = spm.SentencePieceProcessor()\n",
        "en_sp.load('Multi30k_en.model')\n",
        "\n",
        "fr_sp = spm.SentencePieceProcessor()\n",
        "fr_sp.load('Multi30k_fr.model')\n",
        "\n",
        "tokenizers = {\"en\": en_sp.encode_as_ids, \"fr\": fr_sp.encode_as_ids}\n",
        "detokenizers = {\"en\":en_sp.decode_ids, \"fr\":fr_sp.decode_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5cpqK_DJx-Ol"
      },
      "outputs": [],
      "source": [
        "# indexes of special symbols\n",
        "UNK, BOS, EOS, PAD = 0, 1, 2, 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter_en = []\n",
        "with open('data//train.en.txt', encoding='utf-8') as ip_file :\n",
        "  train_iter_en = ip_file.readlines()\n",
        "train_iter_fr = []\n",
        "with open('data//train.fr.txt', encoding='utf-8') as ip_file :\n",
        "  train_iter_fr = ip_file.readlines()\n",
        "\n",
        "valid_iter_en = []\n",
        "with open('data//val.en.txt', encoding='utf-8') as ip_file :\n",
        "  valid_iter_en = ip_file.readlines()\n",
        "valid_iter_fr = []\n",
        "with open('data//val.fr.txt', encoding='utf-8') as ip_file :\n",
        "  valid_iter_fr = ip_file.readlines()\n",
        "\n",
        "test_iter_en = []\n",
        "with open('data//test.en.txt', encoding='utf-8') as ip_file :\n",
        "  test_iter_en = ip_file.readlines()\n",
        "test_iter_fr = []\n",
        "with open('data//test.fr.txt', encoding='utf-8') as ip_file :\n",
        "  test_iter_fr = ip_file.readlines()\n",
        "\n",
        "train_set = list(zip(train_iter_en, train_iter_fr))\n",
        "valid_set = list(zip(valid_iter_en, valid_iter_fr))\n",
        "test_set = list(zip(test_iter_en, test_iter_fr))\n",
        "\n",
        "train_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in train_set if x!='']\n",
        "valid_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in valid_set if x!='']\n",
        "test_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in test_set if x!='']\n",
        "print(len(train_set), len(valid_set), len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSto6MSOMRkh",
        "outputId": "90d65687-b74f-4b77-e1a8-0677fa1d3047"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29000 1014 461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FVVVuXnB24QP"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 50\n",
        "def tokenize_dataset(dataset):\n",
        "    return [(torch.tensor([BOS]+tokenizers[SRC](src_text)[0:MAX_SEQ_LEN-2]+[EOS]),\n",
        "             torch.tensor([BOS]+tokenizers[TRG](trg_text)[0:MAX_SEQ_LEN-2]+[EOS]))\n",
        "            for src_text, trg_text in dataset]\n",
        "\n",
        "train_tokenized = tokenize_dataset(train_set)\n",
        "valid_tokenized = tokenize_dataset(valid_set)\n",
        "test_tokenized  = tokenize_dataset(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Rnv84UM17rEC"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def pad_sequence(batch):\n",
        "    src_seqs  = [src for src, trg in batch]\n",
        "    trg_seqs  = [trg for src, trg in batch]\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs,\n",
        "                                batch_first=True, padding_value = PAD)\n",
        "    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_seqs,\n",
        "                                batch_first=True, padding_value = PAD)\n",
        "    return src_padded, trg_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EJYyg4uS-lKm"
      },
      "outputs": [],
      "source": [
        "class Dataloaders:\n",
        "    def __init__(self):\n",
        "        self.train_dataset = TranslationDataset(train_tokenized)\n",
        "        self.valid_dataset = TranslationDataset(valid_tokenized)\n",
        "        self.test_dataset  = TranslationDataset(test_tokenized)\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True, collate_fn = pad_sequence)\n",
        "\n",
        "\n",
        "        self.valid_loader = torch.utils.data.DataLoader(self.valid_dataset, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True, collate_fn=pad_sequence)\n",
        "\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True, collate_fn=pad_sequence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRelsO2tGckD"
      },
      "source": [
        "**Training and Evaluation functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "niOXK3aHKJ0M"
      },
      "outputs": [],
      "source": [
        "def prepare_batch(x, y):\n",
        "    src = x.to(DEVICE)\n",
        "    tgt = y[:, :-1].to(DEVICE)\n",
        "    tgt_y = y[:, 1:].contiguous().view(-1).to(DEVICE)\n",
        "    return src, tgt, tgt_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Iz1AO6PzPZ6l"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloaders):\n",
        "    model.train()\n",
        "    losses= []\n",
        "    grad_norm_clip = 1.0\n",
        "    for x, y  in  dataloaders.train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        src, trg_in, trg_out = prepare_batch(x,y)\n",
        "        out = model.forward(src, trg_in).to(DEVICE)\n",
        "        out = out.contiguous().view(out.shape[0]*out.shape[1],  -1)\n",
        "        loss = loss_fn(out, trg_out).to(DEVICE)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def validate(model, dataloaders):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloaders.valid_loader:\n",
        "            src, trg_in, trg_out = prepare_batch(x,y)\n",
        "            out = model.forward(src, trg_in).to(DEVICE)\n",
        "            out = out.contiguous().view(out.shape[0]*out.shape[1],  -1)\n",
        "            loss = loss_fn(out, trg_out).to(DEVICE)\n",
        "            losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def train(model, dataloaders, epochs, early_stop_count, warmup_steps):\n",
        "    best_valid_loss = float('inf')\n",
        "    train_size = len(dataloaders.train_loader)*BATCH_SIZE\n",
        "    for ep in range(epochs):\n",
        "        train_loss = train_epoch(model, dataloaders)\n",
        "        valid_loss = validate(model, dataloaders)\n",
        "\n",
        "        print(f'Epoch: {ep}: train_loss={train_loss:.5f}, valid_loss={valid_loss:.5f}')\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            EARLY_STOP = 2\n",
        "        else:\n",
        "            if scheduler.last_epoch>2*warmup_steps:\n",
        "                EARLY_STOP -= 1\n",
        "                if EARLY_STOP<=0:\n",
        "                    return train_loss, valid_loss\n",
        "    return train_loss, valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCsLJ4MutoYB",
        "outputId": "15e9837c-2282-4861-b95e-4583406ddc37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0: train_loss=5.55131, valid_loss=3.53056\n",
            "Epoch: 1: train_loss=2.85581, valid_loss=2.12786\n",
            "Epoch: 2: train_loss=1.84443, valid_loss=1.54125\n",
            "Epoch: 3: train_loss=1.35291, valid_loss=1.27453\n",
            "Epoch: 4: train_loss=1.06789, valid_loss=1.14850\n",
            "Epoch: 5: train_loss=0.89309, valid_loss=1.08078\n",
            "Epoch: 6: train_loss=0.77116, valid_loss=1.05666\n",
            "Epoch: 7: train_loss=0.67675, valid_loss=1.04649\n",
            "Epoch: 8: train_loss=0.59974, valid_loss=1.02408\n",
            "Epoch: 9: train_loss=0.53824, valid_loss=1.01756\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "data_loaders = Dataloaders()\n",
        "MAX_SEQ_LEN = 50\n",
        "\n",
        "EPOCS = 10\n",
        "EARLY_STOP = 1\n",
        "config = {'src_pad_idx':PAD, 'trg_pad_idx':PAD, 'src_vocab_size':vocab_sizes[SRC],\n",
        "          'trg_vocab_size':vocab_sizes[TRG], 'embed_size':512,'num_layers':2,\n",
        "          'forward_expansion':2,'heads':8,'dropout':0.1,'max_length':MAX_SEQ_LEN}\n",
        "model = ModelBuilder(config).make_model()\n",
        "\n",
        "warmup_steps = 3*len(data_loaders.train_loader)\n",
        "lr_fn = lambda step: 512**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "\n",
        "train_loss, valid_loss = train(model, data_loaders, EPOCS, EARLY_STOP, warmup_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eIcLtuSdKgxi"
      },
      "outputs": [],
      "source": [
        "def remove_pad(sent):\n",
        "    if sent.count(EOS)>0:\n",
        "      sent = sent[0:sent.index(EOS)+1]\n",
        "    while sent and sent[-1] == PAD:\n",
        "            sent = sent[:-1]\n",
        "    return sent\n",
        "\n",
        "def decode_sentence(detokenizer, sentence_ids):\n",
        "    if not isinstance(sentence_ids, list):\n",
        "        sentence_ids = sentence_ids.tolist()\n",
        "    sentence_ids = remove_pad(sentence_ids)\n",
        "    return detokenizer(sentence_ids).replace(\"<bos>\", \"\")\\\n",
        "           .replace(\"<eos>\", \"\").strip().replace(\" .\", \".\")\n",
        "\n",
        "def translate(model, x):\n",
        "    with torch.no_grad():\n",
        "        dB = x.size(0)\n",
        "        y = torch.tensor([[BOS]*dB]).view(dB, 1).to(DEVICE)\n",
        "        x_mask = (x != PAD).unsqueeze(1).unsqueeze(2)\n",
        "        enc_op = model.encoder(x, x_mask)\n",
        "        for i in range(MAX_SEQ_LEN):\n",
        "            trg_mask1 = (y != PAD).unsqueeze(1).unsqueeze(2)\n",
        "            N, trg_len = y.shape\n",
        "            trg_mask2 = torch.tril(torch.ones((trg_len, trg_len), dtype=bool)).expand(\n",
        "                N, 1, trg_len,trg_len\n",
        "            ).to(DEVICE)\n",
        "            y_mask = trg_mask1 & trg_mask2\n",
        "            out = model.decoder(y, enc_op, x_mask, y_mask)\n",
        "            out = torch.softmax(out, dim=-1 )\n",
        "            logits = out\n",
        "            last_output = logits.argmax(-1)[:, -1]\n",
        "            last_output = last_output.view(dB, 1)\n",
        "            y = torch.cat((y, last_output), 1).to(DEVICE)\n",
        "    return y\n",
        "\n",
        "def evaluate(model, dataloader, num_batch=None):\n",
        "    model.eval()\n",
        "    refs, cans, bleus = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(dataloader):\n",
        "            src, trg_in, trg_out = prepare_batch(x,y)\n",
        "            out = model.forward(src, trg_in).to(DEVICE)\n",
        "            out = out.contiguous().view(out.shape[0]*out.shape[1],  -1)\n",
        "\n",
        "            # src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n",
        "            translation = translate(model, src)\n",
        "            trg_out = trg_out.view(x.size(0), -1)\n",
        "            refs = refs + [decode_sentence(detokenizers[TRG], trg_out[i]) for i in range(len(src))]\n",
        "            cans = cans + [decode_sentence(detokenizers[TRG], translation[i]) for i in range(len(src))]\n",
        "            if num_batch and idx>=num_batch:\n",
        "                break\n",
        "        print(min([len(x) for x in refs]))\n",
        "        bleus.append(sacrebleu.corpus_bleu(cans, [refs]).score)\n",
        "        # print some examples\n",
        "        for i in range(3):\n",
        "            print(f'src:  {decode_sentence(detokenizers[SRC], src[i])}')\n",
        "            print(f'trg:  {decode_sentence(detokenizers[TRG], trg_out[i])}')\n",
        "            print(f'pred: {decode_sentence(detokenizers[TRG], translation[i])}')\n",
        "            print('*'*50)\n",
        "        return np.mean(bleus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_bleu  = evaluate(model, data_loaders.valid_loader)\n",
        "test_bleu  = evaluate(model, data_loaders.test_loader)\n",
        "print(f'valid_bleu: {valid_bleu:.4f}, test_bleu: {test_bleu:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEUIvL5ESOg3",
        "outputId": "5c3583d8-ddcf-4bcb-d7d3-eee86e250faa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "src:  a man in a brightly-colored ski jacket stands with others on a european street.\n",
            "trg:  un homme avec une veste de ski de couleur vive est debout parmi d&apos; autres dans une rue d&apos; europe.\n",
            "pred: un homme en veste de couleur vive est debout avec d&apos; autres personnes en europe.\n",
            "**************************************************\n",
            "src:  street scene of blond woman in gold coat and pink mini-skirt in front of a rear-facing police motorcycle.\n",
            "trg:  une femme blonde dans une rue , avec un manteau dor√© et une mini-jupe rose devant une moto de police vue de dos.\n",
            "pred: une sc√®ne blonde en manteaux en manteaux dor√© et rouge ray√© devant une moto , faisant de la moto.\n",
            "**************************************************\n",
            "src:  a woman in a restaurant is drinking out of a coconut , using a straw.\n",
            "trg:  une femme dans un restaurant , est en train de boire une noix de coco , √† l&apos; aide d&apos; une paille.\n",
            "pred: une femme dans un restaurant boit une noix de coco , en utilisant une paille.\n",
            "**************************************************\n",
            "28\n",
            "src:  a skier is resting in the middle of a race.\n",
            "trg:  un skieur repose au milieu d&apos; une course.\n",
            "pred: un skieur se repose au milieu d&apos; une course.\n",
            "**************************************************\n",
            "src:  a person in gloves showing the time on his watch\n",
            "trg:  une personne portant des gants montrant le temps sur sa montre\n",
            "pred: une personne en gants montrant le temps sur son temps.\n",
            "**************************************************\n",
            "src:  a giraffe leaning over a fence at a zoo.\n",
            "trg:  une girafe se penchant au dessus d&apos; une cl√¥ture dans un zoo.\n",
            "pred: une cha√Æne de basket-ball se penchant sur une cl√¥ture dans un zoo.\n",
            "**************************************************\n",
            "valid_bleu: 55.9854, test_bleu: 42.9486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SlwW51pPrVPt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "feb75d45-2761-421d-9d74-249fedcf1706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'un homme ouvre un cadeau et pose avec celui-ci pour une photo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "def translate_this_sentence(text: str):\n",
        "    input = torch.tensor([[BOS] + tokenizers[SRC](text) + [EOS]]).to(DEVICE)\n",
        "    output = translate(model, input)\n",
        "    return decode_sentence(detokenizers[TRG], output[0])\n",
        "\n",
        "translate_this_sentence(\"a man is opening a present and posing with it for a picture\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank Youü¶ä"
      ],
      "metadata": {
        "id": "zgRuJipve07I"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}